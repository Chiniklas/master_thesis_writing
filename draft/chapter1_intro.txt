Introduction:

Mastering control over intricate nonlinear systems has been a central pursuit within the realm of control theory for a considerable duration. Over the course of history, control engineers have curated a diverse array of methodologies to grapple with these complex systems. The evolution of technology has played a pivotal role, markedly enhancing their capabilities in navigating these intricacies.

In the early stages, control engineers turned to the technique of approximating nonlinear systems by linearizing them around specific operating points. Linear control theory, especially the widely embraced proportional-integral-derivative (PID) control, found extensive utility in handling these linearized models. However, it soon became evident that this approach had its limitations in accurately portraying the behavior of highly nonlinear systems.

A crucial breakthrough emerged with the introduction of feedback linearizationâ€”a method that adeptly transforms a nonlinear system into a linear approximation through a well-thought-out change of variables. This transformation enabled the application of linear control techniques, facilitating system stabilization and the attainment of desired performance.

Another significant milestone was the inception of adaptive control. Adaptive control methods were specifically devised to grapple with uncertainties and the ever-changing nonlinearities inherent in real-world systems. These methodologies dynamically adjust control parameters based on the system's behavior, accommodating shifts in its dynamics.

Sliding mode control, addressing discontinuous nonlinearities, is achieved by compelling the system's state trajectory to smoothly traverse a predefined manifold. This approach offers robustness against uncertainties and disturbances.

As mathematical prowess and control theory progressed, a plethora of sophisticated techniques tailored explicitly for nonlinear systems surfaced. These included Lyapunov stability theory, backstepping control, and modern nonlinear control paradigms like nonlinear model predictive control (NMPC). These advancements have undeniably enriched the toolkit of control engineers, empowering them to effectively navigate the complexities of nonlinear dynamics.

Motivation:

Robots, which are programmable mechanical entities, are purposefully designed for autonomous or semi-autonomous task execution, showcasing mobility, manipulation, and interaction with their surroundings.

In the realm of modern robotics, these machines exemplify intricate, highly nonlinear mechanical systems. Some well-known instances include quadruped robotics, autonomous vehicles, quadcopters, and humanoid robots.

[Images of the mentioned robots can be inserted here]

To enhance their motion capabilities, sophisticated nonlinear control methods are essential. Traditionally, for planning complex motion in systems like those mentioned above, a two-step approach is followed, namely trajectory planning and trajectory tracking.

Trajectory planning involves computing a smooth and feasible path that a robot should follow to reach a specific target position or work point. A common method employed is trajectory optimization, aiming to minimize a cost function that factors in travel time, energy consumption, and smoothness of motion. Techniques such as gradient descent or genetic algorithms are often used for this optimization. Adhering to constraints such as maximum velocity and accelerations is crucial during this process.

Once the trajectory is successfully planned, trajectory tracking involves implementing control algorithms to guide the robot along the planned trajectory. Typically, a feedback control approach is utilized, continuously monitoring the robot's position and adjusting control inputs. However, in real-world systems, even with a well-planned trajectory, external disturbances, uncertainties, or system limitations may cause significant deviations. Hence, accurate state estimation and robust control are crucial in the realm of feedback control.

In the domain of industrial robot control, it's typical to separate the process into distinct planning and execution phases. This division arises from the fact that real-time responsiveness is not a stringent necessity in this context. When a task is defined, the planning phase kicks in, utilizing algorithms like linear interpolation and A* for trajectory generation. Following this, a steady and reliable control policy is implemented to ensure precise tracking of the generated trajectory during the execution phase.

Yet, in dynamic domains like automotive and flight control, the demand for real-time responsiveness takes center stage. Model Predictive Control (MPC) stands as a prime illustration of this critical requirement. MPC seamlessly integrates trajectory planning and execution into a unified framework. The process commences by projecting a sequence of control actions into the future as part of the planning stage. Subsequently, the calculated control inputs are meticulously fine-tuned to minimize the deviation between the actual system state and the planned trajectory. This strategic implementation effectively guides the system to closely track the intended trajectory.

MPC's brilliance lies in its ability to concurrently devise and optimize trajectories while swiftly adapting in real time to stay closely aligned with the planned trajectory, even when facing various disturbances and uncertainties. This amalgamation plays a pivotal role in achieving precise control and adaptability, especially in rapidly changing and intricate environments.

Though the traditional approach of trajectory generation and tracking has proven effective for many systems, but it has the following drawbacks:
- Limited adaptibility
Trajectory planning typically relies on predefined paths or trajectories, limiting adaptability to unforseenable changes or dynamical environments. If the environment changes significantly, the planned trajectory may no longer be optimal or even feasible.

- Difficulty in Complex Environments:
In highly complex and cluttered environments, planning a feasible trajectory that avoids obstacles while reaching the goal can be challenging. The complexity increases with the number of obstacles and the intricacy of the environment.

- Difficulty with Nonlinear Systems:
Trajectory planning struggles with highly nonlinear systems where the dynamics are hard to model accurately. Linearizing the system for planning purposes may lead to suboptimal or infeasible trajectories.

- Static Planning:
Traditional trajectory planning is often static, assuming a stationary environment. It does not readily adapt to changing circumstances or dynamic obstacles, which limits its applicability in real-world scenarios.

- High Computational Demands:
Some trajectory planning algorithms can be computationally intensive, especially for high-dimensional or complex robotic systems. This computational demand becomes a drawback, particularly in real-time or time-critical applications.





In contrast to trajectory-based control, reinforcement learning (RL)-based control extracts an optimal policy through interactions with the environment, offering several advantages:

Adaptability and Flexibility:
RL enables systems to adapt and learn optimal behavior in environments that are dynamic and prone to change. The control policy can continuously evolve based on new experiences and acquired knowledge, ensuring adaptability to varying circumstances.

No Explicit Model Required:
In contrast to traditional control methods that often necessitate a precise mathematical model of the system, RL can directly learn from interactions with the environment without relying on an explicit model. This characteristic is particularly valuable in scenarios where system dynamics are complex or unknown.

Effective Handling of Nonlinearities and Complex Systems:
RL proves highly effective in dealing with highly nonlinear systems and complex control tasks that might pose challenges for traditional control methods. The use of neural network-based function approximation allows for the capture of intricate relationships between states and actions.

Efficient Handling of High-Dimensional Input Spaces:
RL demonstrates an ability to efficiently manage high-dimensional and continuous input spaces, a crucial feature in many real-world applications such as robotics, finance, and game playing.

Reinforcement learning, by learning directly from the environment, offers a dynamic and adaptable approach to control, making it particularly suitable for complex and evolving systems.


My main contribution in this paper is to successfully design a control strategy to accomplishing two primary tasks: swinging the double pendulum from its lowest point to the highest point, and maintaining stability at the highest point. To achieve the swing-up task, we employed the classical model-free reinforcement learning algorithm known as soft actor critic to train a policy which is able to reach the region of attraction of a continuous time linear quadratic regulator controller. As soon as the system enters the RoA, we transition to the LQR controller to stabilize the entire system.







